{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rain in Australia: Predict rain tomorrow in Australia\n",
    "### Monica\n",
    "\n",
    "Date: 18-10-2019\n",
    "\n",
    "Version: 3.1.2\n",
    "\n",
    "Environment: Python 3.6, Jupyter Notebook\n",
    "\n",
    "## Libraries used:\n",
    "* matplotlib\n",
    "* pyspark.ml\n",
    "* pyspark.sql\n",
    "* pyspark\n",
    "\n",
    "## Datasets used:\n",
    "* Rain in Australia (__weatherAUS.csv__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A. Creating Spark Session and Loading the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 01: Import Spark Session and initialize Spark\n",
    "\n",
    "* __PySpark__ is the Python API written in python to support Apache Spark.\n",
    "* __SparkSession__ is a unified entry point of a spark application from Spark 2.0. It provides a way to interact with various spark’s functionality with a lesser number of constructs.\n",
    "* __SparkContext__ is the entry point to any spark functionality. When we run any Spark application, a driver program starts, which has the main function and your SparkContext gets initiated here. The driver program then runs the operations inside the executors on worker nodes.\n",
    "* __builder__ is used to set up or construct SparkSession instances.\n",
    "* __master__ is the URL of the cluster it connects to. We use a local server with as many working processors (or threads) as possible (i.e. 'local[k]'). If we want Spark to run locally with 'k' worker threads, we can specify as 'local[k]'.\n",
    "* __local[4]__, Spark is run locally with as many processors as logical cores in the current running machine, in this case 4. This is assigned to \"master\" as it  uses local server with as many logical processors or core available.\n",
    "* __appName()__ is the name of the job or application. It is displayed in the Sparking cluster UI.\n",
    "* __getOrCreate()__ is added to avoid an error : \"Cannot run multiple SparkContexts at once\". If there is an existing SparkContext, we will reuse it instead of creating a new context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-10-27-161-78.ap-southeast-2.compute.internal:4042\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[3]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>FIT5202 Assessment2:Predicting Rain</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f3d95ec9c50>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# creating entry points to spark\n",
    "\n",
    "# Spark\n",
    "from pyspark import SparkContext\n",
    "# Spark Sql\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Creating RDDs with 4 local cores\n",
    "# Building a new SparkSession with 4 local cores\n",
    "spark = SparkSession.builder.\\\n",
    "        master(\"local[3]\").\\\n",
    "        appName('FIT5202 Assessment2:Predicting Rain').\\\n",
    "        getOrCreate()\n",
    "\n",
    "sc = SparkContext.getOrCreate()\n",
    "# displaying the SparkSession/SparkContext\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x=sc.parallelize([1,2])\n",
    "x.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 02: Load the dataset and print the schema and total number of entries\n",
    "\n",
    "* Load the dataset 'weatherAUS.csv'\n",
    "* Print the Schema of the Dataset\n",
    "* Calculating the Toatl Number of entries in the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading the dataset into python\n",
    "data=spark.read.csv('weatherAUS.csv',inferSchema=True,header=True)\n",
    "\n",
    "# printing the schema of the dataset\n",
    "print('Schema of the Dataset:')\n",
    "data.printSchema()\n",
    "\n",
    "# printing the total number of entries\n",
    "print('Total Number of Entries in the Dataset:',data.count())\n",
    "print('Total Number of Columns in the Dataset:',len(data.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying a sample record\n",
    "data.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B. Data Cleaning and Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 03: Delete columns from the dataset\n",
    "Delete unnecessary data from the dataset to improve the efficiency and accuracy of the model.\n",
    "* Date<br>\n",
    "* Location<br>\n",
    "* Evaporation<br>\n",
    "* Sunshine<br>\n",
    "* Cloud9am<br>\n",
    "* Cloud3pm<br>\n",
    "* Temp9am<br>\n",
    "* Temp3pm<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_columns_list=['Date','Location','Evaporation','Sunshine','Cloud9am','Cloud3pm','Temp9am','Temp3pm']\n",
    "present_columns_list=data.columns\n",
    "\n",
    "print(\"Number of Columns to Delete:\",len(delete_columns_list))\n",
    "print(\"Number of Columns Originally:\",len(present_columns_list),\"\\n\")\n",
    "\n",
    "dataStep3 = data\n",
    "for item in delete_columns_list:\n",
    "    if item in present_columns_list:\n",
    "        print(\"......Deleting\",item)\n",
    "        dataStep3 = dataStep3.drop(item)  \n",
    "\n",
    "print(\"\\nColumns Left after Deletion:\",len(dataStep3.columns),\"\\n\")\n",
    "print(dataStep3.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying a sample record after deleting the columns mentioned above\n",
    "dataStep3.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 04: Print the number of missing data in each column\n",
    "* We can observe that there are lots of NA (null) values in the given dataset.\n",
    "* Print the number of NA(null) values in each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count, when\n",
    "\n",
    "# just out of intuition, checking for null values\n",
    "print(\"Number of Null Values in every Column:\\n\")\n",
    "dataStep3.select([count(when(dataStep3[c].isNull(),1)).alias(c) for c in dataStep3.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not have any null values in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining a function for calculating the missing data in each column and displaying the correct number\n",
    "def missing_data_function(dataset):\n",
    "    print(\"Number of Missing Data or NA values in every Column:\\n\")\n",
    "    dataset.select([count(when(dataset[c] == \"NA\",1)).\\\n",
    "                      alias(c) for c in dataset.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calling the function to calculate number of missing values in each column\n",
    "missing_data_function(dataStep3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from the above observations we can see that __'Pressure9am'__ has the highest number of missing values and __'MaxTemp'__ has the lowest number of missing values. Apparently, it is good to see that out target variable __'RainTomorrow'__ has no missing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 05: Fill the missing data with average value and maximum occurrence value\n",
    "\n",
    "* Fill in all the missing data with average value (for numeric column) or maximum frequency value (for non-numeric column).\n",
    "* Identify the columns which have numeric values (e.g., MinTemp, MaxTemp), calculate the average and fill the null value with the average.\n",
    "* Identify the columns with non-numeric values (e.g., WindGustDir, WindDir9am) and find the most frequent item (e.g., wind direction) and fill the null values with that item for that particular column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the average of a column and filling the null values with the average, for columns with Numeric Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean\n",
    "\n",
    "dataStep5 = dataStep3\n",
    "\n",
    "# numeric data columns\n",
    "double_columns_list = ['MinTemp','MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm',\n",
    "                       'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm']\n",
    "\n",
    "# calculating mean of every numeric data column\n",
    "meanList = dataStep5.select([mean(c).alias(c) for c in double_columns_list])\n",
    "mean_columns = meanList.collect()\n",
    "\n",
    "# displaying mean of every numeric data column\n",
    "meanList.show(vertical=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values with the mean as found above\n",
    "for i in range(len(double_columns_list)):\n",
    "    dataStep5 = dataStep5.withColumn(double_columns_list[i],when(dataStep5[double_columns_list[i]] == 'NA', \n",
    "                                        mean_columns[0][i]).otherwise(dataStep5[double_columns_list[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# checking if missing values of numeric data columns has been replaced or not, \n",
    "# by displaying the number of missing values for those columns\n",
    "dataStep5.select([count(when(dataStep5[c] == \"NA\",1)).\\\n",
    "                  alias(c) for c in dataStep5.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have replaced all missing values with the average of the numeric data columns, and hence we do not have any missing values for the respective columns as observed above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding the most frequent item in a column and filling the null values with that item, for columns with non-numeric values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# non numeric data columns\n",
    "string_columns_list = ['WindGustDir','WindDir9am','WindDir3pm','RainToday','RainTomorrow']\n",
    "row = []\n",
    "\n",
    "# calculating the most frequent item in non numeric data columns\n",
    "for item in string_columns_list:\n",
    "    row.append(dataStep5.groupby(item).count().sort('count').collect()[-1])\n",
    "\n",
    "row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filling missing values with the most frequent item as found above\n",
    "for i in range(len(string_columns_list)):\n",
    "    dataStep5 = dataStep5.withColumn(string_columns_list[i],when(dataStep5[string_columns_list[i]] == 'NA', \n",
    "                                                row[i][0]).otherwise(dataStep5[string_columns_list[i]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# checking if missing values of non numeric data columns has been replaced or not, \n",
    "# by displaying the number of missing values for those columns\n",
    "dataStep5.select([count(when(dataStep5[c] == \"NA\",1)).\\\n",
    "                  alias(c) for c in dataStep5.columns]).show(vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hence, we have replaced all missing values of the non-numeric columns as well and as observed above we do not have any missing values for all the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 06: Data transformation\n",
    "\n",
    "* Transform the data so that it will be useful to process by the machine learning algorithm.\n",
    "* Before transforming the non-numerical data, perform type casting (to double) of the numerical value columns as they are defined as “String” (see, the schema of the dataset)\n",
    "* For the non-numerical value column (i.e., WindGustDir, WindDir9am, WindDir3pm, RainTomorrow) use the StringIndexer method to convertthem into numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# displaying the schema of the dataset\n",
    "dataStep5.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As mentioned in the question, all the numerical data columns are defined as __'String'__."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type casting (to double) of the numerical value columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "dataStep6 = dataStep5\n",
    "\n",
    "# type casting to double of the numeric data columns\n",
    "for item in double_columns_list:\n",
    "    dataStep6 = dataStep6.withColumn(item,dataStep6[item].cast(DoubleType()))\n",
    "        \n",
    "# displaying the schema after type casting to double\n",
    "dataStep6.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the StringIndexer method to convert non-numerical value columns into numbers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoderEstimator, StringIndexer\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# initializing numeric and non-numeric data columns again\n",
    "numericCols = ['MinTemp','MaxTemp','Rainfall','WindGustSpeed','WindSpeed9am','WindSpeed3pm',\n",
    "                       'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm']\n",
    "\n",
    "categoricalColumns = ['WindGustDir','WindDir9am','WindDir3pm','RainToday']\n",
    "\n",
    "stages = []\n",
    "\n",
    "for categoricalCol in categoricalColumns:\n",
    "    stringIndexer = StringIndexer(inputCol = categoricalCol, outputCol = categoricalCol + 'Index')\n",
    "    \n",
    "    # Convert indexed categories to one-hot encoded variables (classVec)\n",
    "    encoder = OneHotEncoderEstimator(inputCols=[stringIndexer.getOutputCol()], \n",
    "                                     outputCols=[categoricalCol + \"classVec\"])\n",
    "    \n",
    "    # When printing steps, a binary vector is added to the end of each line.\n",
    "    stages += [stringIndexer, encoder]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a pipeline for the string indexer stages\n",
    "pipeline = Pipeline(stages = stages)\n",
    "\n",
    "# fitting the dataset to the pipeline\n",
    "pipelineModel = pipeline.fit(dataStep6)\n",
    "\n",
    "# transforming the data using the pipeline model\n",
    "dataStep6 = pipelineModel.transform(dataStep6)\n",
    "\n",
    "# displaying one record \n",
    "dataStep6.show(1,vertical=True)\n",
    "\n",
    "# displaying the schema after String Indexing the non-numeric data column\n",
    "dataStep6.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, as observed in the present schema above, along with the StringIndexer of the non-nomerical columns, Class Vector of the respective non-numerical data columns has also been created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping the non-numeric indexed columns along with numeric columns only,\n",
    "# dropping the unnecessary columns\n",
    "\n",
    "drop_columns_list = ['WindGustDir','WindDir9am','WindDir3pm','RainToday','WindGustDirclassVec',\n",
    "                    'WindDir9amclassVec','WindDir3pmclassVec','RainTodayclassVec']\n",
    "\n",
    "for item in drop_columns_list:\n",
    "    if item in dataStep6.columns:\n",
    "        dataStep6 = dataStep6.drop(item)  \n",
    "\n",
    "# rearranging the columns\n",
    "dataStep6 = dataStep6.select('MinTemp','MaxTemp','Rainfall','WindGustDirIndex','WindGustSpeed',\n",
    "                             'WindDir9amIndex','WindDir3pmIndex','WindSpeed9am','WindSpeed3pm',\n",
    "                             'Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','RainTodayIndex',\n",
    "                             'RainTomorrow')\n",
    "dataStep6.show(1,vertical=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 07: Create the feature vector and divide the dataset\n",
    "* create the feature vector from the given columns. When creating a feature vector, remember to exclude the column that will be using for testing the accuracy of your model.\n",
    "* After creation of the feature vector, split the dataset into two (e.g., training and testing). In this assignment, spit the dataset randomly and between 70 percent and 30 percent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the Feature Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "\n",
    "dataStep7 = dataStep6\n",
    "\n",
    "# excluding 'label_RainTomorrow' which wil be used for testing the accuracy of the model\n",
    "assemblerInputs = ['MinTemp','MaxTemp','Rainfall','WindGustDirIndex','WindGustSpeed',\n",
    "                   'WindDir9amIndex','WindDir3pmIndex','WindSpeed9am',\n",
    "                   'WindSpeed3pm','Humidity9am','Humidity3pm','Pressure9am','Pressure3pm','RainTodayIndex']\n",
    "\n",
    "# creating feature vector \n",
    "assembler = VectorAssembler(inputCols = assemblerInputs, outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transforming the dataset using VectorAssembler object\n",
    "dataStep7 = assembler.transform(dataStep7)\n",
    "\n",
    "# displaying a record to show its feature vector\n",
    "dataStep7.show(1,vertical=True,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# string indexing the label/target column\n",
    "label_stringIdx = StringIndexer(inputCol = 'RainTomorrow', outputCol = 'label')\n",
    "\n",
    "# transforming the dataset using StringIndexer object\n",
    "dataStep7 = label_stringIdx.fit(dataStep7).transform(dataStep7)\n",
    "\n",
    "# displaying the feature vector along with the columns 'RainTomorrow' and 'label_RainTomorrowIndex'\n",
    "dataStep7.select('RainTomorrow','features','label').show(10,truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping the original column of the label i.e. RainTomorrow\n",
    "dataStep7 = dataStep7.drop('RainTomorrow')\n",
    "\n",
    "# displaying a record of the finalised dataset\n",
    "dataStep7.show(1,truncate=False, vertical = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dividing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainingData, testData = dataStep7.randomSplit([0.7, 0.3], seed = 2018)\n",
    "print(\"Training Dataset Count: \" + str(trainingData.count()))\n",
    "print(\"Test Dataset Count: \" + str(testData.count()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C. Apply Machine Learning Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 08: Apply machine learning classification algorithms on the dataset and compare their accuracy. Plot the accuracy as bar graph.\n",
    "* Use DecisionTreeClassifier(), RandomForestClassifier(), and LogisticRegression(), GBTClassifier() methods in spark to calculate the probability of the rain fall tomorrow based on the other related data points (e.g., temperature, wind, humidity). \n",
    "\n",
    "* Finally, draw the graph (e.g. bar chart) to demonstrate the comparison of their accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# importing the librraies\n",
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "# building a decision tree classifier model\n",
    "descTree = DecisionTreeClassifier(labelCol=\"label\", featuresCol=\"features\")\n",
    "\n",
    "# fitting the model in the dataset\n",
    "modelDT = descTree.fit(trainingData)\n",
    "\n",
    "# predicting the test data with the fitted model\n",
    "predictionsDT = modelDT.transform(testData)\n",
    "\n",
    "# displaying the predictions and the actual values of the target variable\n",
    "predictionsDT.select(\"prediction\", \"label\").show(5)\n",
    "\n",
    "# calculating the accuracy of the model\n",
    "evaluatorDT1 = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                                                 predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracyDT = evaluatorDT1.evaluate(predictionsDT)\n",
    " \n",
    "print(\"Using DecisionTreeClassifier->\\n\")\n",
    "print(\" * Accuracy:\", round(accuracyDT*100,4),\"%\\n\")\n",
    "print(\" * Test Error:\",round(1.0 - accuracyDT,4),\"\\n\")\n",
    "\n",
    "# calculating the area under ROC of the model\n",
    "evaluatorDT2 = BinaryClassificationEvaluator()\n",
    "print(' * Test Area Under ROC: '+ str(evaluatorDT2.evaluate(predictionsDT, \n",
    "                                                          {evaluatorDT1.metricName: \"areaUnderROC\"}))+'\\n')\n",
    "\n",
    "# calculating the probability of rainfall tomorrow based on 'MaxTemp','WindSpeed9am','Humidity9am','Pressure9am'\n",
    "predictionsDT.select('MaxTemp','WindSpeed9am','Humidity9am','Pressure9am',\\\n",
    "                     'label','rawPrediction',\n",
    "                     'prediction', 'probability').show(5, vertical=True, truncate= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the librraies\n",
    "from pyspark.ml.classification import RandomForestClassifier\n",
    "\n",
    "# building a random forest classifier model\n",
    "ranFor = RandomForestClassifier(labelCol=\"label\",featuresCol=\"features\", numTrees=10)\n",
    "\n",
    "# fitting the model in the dataset\n",
    "modelRF = ranFor.fit(trainingData)\n",
    "\n",
    "# predicting the test data with the fitted model\n",
    "predictionsRF = modelRF.transform(testData)\n",
    "\n",
    "# displaying the predictions and the actual values of the target variable\n",
    "predictionsRF.select(\"prediction\", \"label\").show(5)\n",
    "\n",
    "# calculating the accuracy of the model\n",
    "evaluatorRF1 = MulticlassClassificationEvaluator(labelCol=\"label\",\n",
    "                                              predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "accuracyRF = evaluatorRF1.evaluate(predictionsRF)\n",
    "\n",
    "print(\"Using RandomForestClassifier->\\n\")\n",
    "print(\" * Accuracy:\", round(accuracyRF*100,4),\"%\\n\")\n",
    "print(\" * Test Error:\",round(1.0 - accuracyRF,4),\"\\n\")\n",
    "\n",
    "# calculating the area under ROC of the model\n",
    "evaluatorRF2 = BinaryClassificationEvaluator()\n",
    "print(' * Test Area Under ROC '+ str(evaluatorRF2.evaluate(predictionsRF, \n",
    "                                                          {evaluatorRF2.metricName: \"areaUnderROC\"}))+'\\n')\n",
    "\n",
    "# calculating the probability of rainfall tomorrow based on 'MaxTemp','WindSpeed9am','Humidity9am','Pressure9am'\n",
    "predictionsRF.select('MaxTemp','WindSpeed9am','Humidity9am','Pressure9am',\\\n",
    "                     'label','rawPrediction',\n",
    "                     'prediction', 'probability').show(5, vertical = True, truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# building a logistic regression model\n",
    "logReg = LogisticRegression(featuresCol = 'features', labelCol = 'label',    maxIter=10)\n",
    "\n",
    "# fitting the model in the dataset\n",
    "modelLR = logReg.fit(trainingData)\n",
    "\n",
    "# Logistic Regression Model to get the beta coefficients\n",
    "beta = np.sort(modelLR.coefficients)\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(beta)\n",
    "plt.ylabel('Beta Coefficients',fontsize=12)\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicting the test data with the fitted model\n",
    "predictionsLR = modelLR.transform(testData)\n",
    "\n",
    "# displaying the predicted and the actual values of the target variable\n",
    "predictionsLR.select(\"prediction\", \"label\").show(5)\n",
    "\n",
    "# calculating the accuracy of the model\n",
    "evaluatorLR1 = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                              predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracyLR = evaluatorLR1.evaluate(predictionsLR)\n",
    "\n",
    "print(\"Using Logistic Regression->\\n\")\n",
    "print(\" * Accuracy:\", round(accuracyLR*100,4),\"%\\n\")\n",
    "print(\" * Test Error:\",round(1.0 - accuracyLR,4),\"\\n\")\n",
    "\n",
    "# calculating the area under ROC of the model\n",
    "evaluatorLR2 = BinaryClassificationEvaluator()\n",
    "print(' * Test Area Under ROC '+ str(evaluatorLR2.evaluate(predictionsLR, \n",
    "                                                       {evaluatorLR2.metricName: \"areaUnderROC\"}))+'\\n')\n",
    "\n",
    "\n",
    "# calculating the probability of rainfall tomorrow based on 'MaxTemp','WindSpeed9am','Humidity9am','Pressure9am'\n",
    "predictionsLR.select('MaxTemp','WindSpeed9am','Humidity9am','Pressure9am',\\\n",
    "                     'label','rawPrediction',\n",
    "                     'prediction', 'probability').show(5, vertical = True, truncate =False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the libraries\n",
    "from pyspark.ml.classification import GBTClassifier\n",
    "\n",
    "# building a gradient boosted tree classifier model\n",
    "graBooTree = GBTClassifier(maxIter=10)\n",
    "\n",
    "# fitting the model in the dataset\n",
    "modelGBT = graBooTree.fit(trainingData)\n",
    "\n",
    "# predicting the test data with the fitted model\n",
    "predictionsGBT = modelGBT.transform(testData)\n",
    "\n",
    "# displaying the predictions and the actual values of the target variable\n",
    "predictionsGBT.select(\"prediction\", \"label\").show(5)\n",
    "\n",
    "# calculating the accuracy of the model\n",
    "evaluatorGBT1 = MulticlassClassificationEvaluator(labelCol=\"label\", \n",
    "                                              predictionCol=\"prediction\",metricName=\"accuracy\")\n",
    "accuracyGBT = evaluatorGBT1.evaluate(predictionsGBT)\n",
    "\n",
    "print(\"Using Gradient-Boosted Tree Classifier->\\n\")\n",
    "print(\" * Accuracy:\", round(accuracyGBT*100,4),\"%\\n\")\n",
    "print(\" * Test Error:\",round(1.0 - accuracyGBT,4),\"\\n\")\n",
    "\n",
    "evaluatorGBT2 = BinaryClassificationEvaluator()\n",
    "print(' * Test Area Under ROC '+ str(evaluatorGBT2.evaluate(predictionsGBT, \n",
    "                                                       {evaluatorGBT2.metricName: \"areaUnderROC\"}))+'\\n')\n",
    "\n",
    "# calculating the probability of rainfall tomorrow based on 'MaxTemp','WindSpeed9am','Humidity9am','Pressure9am'\n",
    "predictionsGBT.select('MaxTemp','WindSpeed9am','Humidity9am','Pressure9am',\\\n",
    "                     'label','rawPrediction',\n",
    "                     'prediction', 'probability').show(5, vertical = True, truncate =False)\n",
    "\n",
    "# print(\"\\n\"+ graBooTree.explainParams())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graph to demonstrate the comparison of the Accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# setting figure size\n",
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "classifier = ['Decision Tree','Random Forest','Logistic Regression','Gradient Boosted Tree']\n",
    "accuracy = [accuracyDT,accuracyRF,accuracyLR,accuracyGBT]\n",
    "accuracy = [item*100 for item in accuracy]\n",
    "bar_label = [str(round(accuracyDT*100,4))+\"%\",\n",
    "             str(round(accuracyRF*100,4))+\"%\",\n",
    "             str(round(accuracyLR*100,4))+\"%\",\n",
    "             str(round(accuracyGBT*100,4))+\"%\"]\n",
    "\n",
    "# plotting the bar graph of accuracies\n",
    "bar_plot = plt.bar(classifier,accuracy,width=0.5,color='blue')\n",
    "\n",
    "# adjusting the axis\n",
    "ax=plt.gca()\n",
    "ax.set_ylim(80,84.5)\n",
    "\n",
    "# labelling the plt\n",
    "plt.xlabel('Classifier',fontsize=15)\n",
    "plt.ylabel('Accuracy',fontsize=15)\n",
    "plt.title(\"Accuracies of different Classifiers\",fontsize=19)\n",
    "\n",
    "def autolabel(bar_plot):\n",
    "    for idx,rect in enumerate(bar_plot):\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2.,height,\n",
    "               bar_label[idx],\n",
    "               ha='center',va='bottom',rotation=0)\n",
    "\n",
    "autolabel(bar_plot)\n",
    "\n",
    "print('\\nAccuracy of Decision Tree Classifier          :',round(accuracy[0],4),\"%\")\n",
    "print('Accuracy of Random Forest Classifier          :',round(accuracy[1],4),\"%\")\n",
    "print('Accuracy of Logistic Regression               :',round(accuracy[2],4),\"%\")\n",
    "print('Accuracy of Gradient Boosted Tree Classifier  :',round(accuracy[3],4),\"%\\n\")\n",
    "\n",
    "# displaying the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 09: Calculate the confusion matrix and find the precision, recall, and F1 score of each classification algorithm. Explain how the accuracy of the predication can be improved?\n",
    "\n",
    "Apart from the accuracy, number of false positive and false negative identification also plays an important role on deciding the quality of any particular classification model. The way we can calculate is called confusion matrix.\n",
    "\n",
    "* Calculate Confusion Matrix\n",
    "* Find Precision, Recall and F1 score\n",
    "* Explain how the accuracy of the prediction can be improved"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Definition of the terms:__\n",
    "* Positive (P) : Observation is positive.\n",
    "* Negative (N) : Observation is not positive.\n",
    "* True Positive (TP) : Observation is positive, and is predicted to be positive.\n",
    "* False Negative (FN) : Observation is positive, but is predicted negative.\n",
    "* True Negative (TN) : Observation is negative, and is predicted to be negative.\n",
    "* False Positive (FP) : Observation is negative, but is predicted positive.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Confusion Matrix:__ A confusion matrix is a summary of prediction results on a classification problem.\n",
    "The number of correct and incorrect predictions are summarized with count values and broken down by each class. This is the key to the confusion matrix.<br>\n",
    "\n",
    "\n",
    "$ \\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;Pred:No\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;Pred:Yes $\n",
    "\n",
    "\n",
    "$ \\;\\;True:No\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;TN\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;FP $\n",
    "\n",
    "\n",
    "$ \\;\\;True:Yes\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;FN\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;\\;TP $<br>\n",
    "\n",
    "\n",
    "__Recall:__ Recall can be defined as the ratio of the total number of correctly classified positive examples divide to the total number of positive examples. High Recall indicates the class is correctly recognized (small number of FN).\n",
    "$$ Recall = TP/(TP+FN) $$\n",
    "\n",
    "__Precision:__ To get the value of precision we divide the total number of correctly classified positive examples by the total number of predicted positive examples. High Precision indicates an example labeled as positive is indeed positive (small number of FP).\n",
    "$$ Precision = TP/(TP+FP) $$\n",
    "\n",
    "__High recall, low precision:__ This means that most of the positive examples are correctly recognized (low FN) but there are a lot of false positives.\n",
    "\n",
    "__Low recall, high precision:__ This shows that we miss a lot of positive examples (high FN) but those we predict as positive are indeed positive (low FP).\n",
    "\n",
    "__F1-measure:__ Since we have two measures (Precision and Recall) it helps to have a measurement that represents both of them. We calculate an F-measure which uses Harmonic Mean in place of Arithmetic Mean as it punishes the extreme values more. The F-Measure will always be nearer to the smaller value of Precision or Recall.\n",
    "$$ F1-measure = (2*Recall*Precision)/(Recall+Precision) $$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.evaluation import MulticlassMetrics\n",
    "\n",
    "# defining function for confusion matrix, precision, recall and F1 score (in built functions)\n",
    "def printMetrics(predictions_and_labels):\n",
    "    metrics = MulticlassMetrics(predictions_and_labels)\n",
    "    confusion_matrix = metrics.confusionMatrix()\n",
    "    print(\"\\n------------------------Inbuilt Calculation of Metrics--------------------\")\n",
    "    print('\\nConfusion Matrix\\n', confusion_matrix)\n",
    "    print('Precision           ', metrics.precision(1))\n",
    "    print('Recall              ', metrics.recall(1))\n",
    "    print('F-1 Score           ', metrics.fMeasure(1.0))\n",
    "\n",
    "# defining function for manually calculating precision from the \n",
    "def precision_calc(tp,fp):\n",
    "    precision = tp / (tp + fp)\n",
    "    return precision\n",
    "\n",
    "# defining function for manually calculating recall\n",
    "def recall_calc(tp,fn):\n",
    "    recall = tp / (tp + fn)\n",
    "    return recall\n",
    "\n",
    "# defining function for manually calculating F1 measure\n",
    "def F1_calc(precision,recall):\n",
    "    F1 = 2 * (precision * recall) / (precision + recall)\n",
    "    return F1\n",
    "\n",
    "# defining function for manual calculation of the metrics from the confusion matrix\n",
    "def printMetrics_manual(predictions_and_labels):\n",
    "    confusion_matrix = MulticlassMetrics(predictions_and_labels).confusionMatrix().toArray()\n",
    "    precision = precision_calc(confusion_matrix[1][1],confusion_matrix[0][1])\n",
    "    recall = recall_calc(confusion_matrix[1][1],confusion_matrix[1][0])\n",
    "    F1 = F1_calc(precision,recall)\n",
    "    print(\"\\n------------------------Manual Calculation of Metrics--------------------\")\n",
    "    print(\"Confusion Matrix: \\n\",confusion_matrix)\n",
    "    print(\"\\nPrecision     \",precision)\n",
    "    print(\"Recall        \",recall)\n",
    "    print(\"F1 Measure    \",F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_and_labels = predictionsDT.select('prediction','label').rdd\n",
    "\n",
    "# calculating metrics manually from the confusion matrix\n",
    "printMetrics_manual(predictions_and_labels)\n",
    "\n",
    "# calculating metrics from the inbuilt function for precision, recall, F1 score:\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RandomForestClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_and_labels = predictionsRF.select('prediction','label').rdd\n",
    "\n",
    "# calculating metrics manually from the confusion matrix\n",
    "printMetrics_manual(predictions_and_labels)\n",
    "\n",
    "# calculating metrics from the inbuilt function for precision, recall, F1 score:\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_and_labels = predictionsLR.select('prediction','label').rdd\n",
    "\n",
    "# calculating metrics manually from the confusion matrix\n",
    "printMetrics_manual(predictions_and_labels)\n",
    "\n",
    "# calculating metrics from the inbuilt function for precision, recall, F1 score:\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GBTClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_and_labels = predictionsGBT.select('prediction','label').rdd\n",
    "\n",
    "# calculating metrics manually from the confusion matrix\n",
    "printMetrics_manual(predictions_and_labels)\n",
    "\n",
    "# calculating metrics from the inbuilt function for precision, recall, F1 score:\n",
    "printMetrics(predictions_and_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Prediction of the accuracy can be improved by the following ways:\n",
    "\n",
    "* Getting rid of the Outliers:<br>\n",
    "Presence of outliers generally degrades the model's accuracy. The model becomes a more bias model which lead to inaccurate predictions. It is important to treat outliers in the dataset by deleting the observations, performing transformation, binning, imputation or by treating outlier values separately.\n",
    "* Get more data or stratified samples: <br>\n",
    "Getting more data is always good better, if it comes to improve the accuracy of the model. Less data always add on more assumptions and weak correlations. Hence having a larger dataset has always led to improvement of the accuracy of the model.\n",
    "* Missing Values should be treated appropriately:<br>\n",
    "\"Similar to outliers, missing values can also degrade the quality and accuracy of the model. We can treat missing values of continuous data by imputing mean, median, mode. For categorical variables, we can treat variables as a separate class. A model can also be build to predict the missing values. KNN imputation offers a great option to deal with missing values.\"\n",
    "* Feature Transformation: <br>\n",
    "\"Feature Transformation also helps to improve the accuracy of the model. We can do feature transformation by standardization or normalization of the variables. Normalization is changing the scale of the variable from 0 to 1. Also, we can transform the variables using log, square root or inverse of the value of the variable to remove skewness. We must remove skewness from the dataset. Some times, creating bins of numeric data works well, since it handles the outlier values also. Numeric data can be made discrete by grouping values into bins. This is known as data discretization.\"\n",
    "* Creating Feautures properly:<br>\n",
    "\"Deriving new variable(s ) from existing variables is known as feature creation. It helps to unleash the hidden relationship of a data set. Let’s say, we want to predict the number of transactions in a store based on transaction dates. Here transaction dates may not have direct correlation with number of transaction, but if we look at the day of a week, it may have a higher correlation. In this case, the information about day of a week is hidden. We need to extract it to make the model better.\"\n",
    "* Correlation between the features: <br>\n",
    "\"We shoild also take into consideration the correlation of the predictor variables among themselves i.e pair wise correlation. Correlated variables add no importance to the model. This can be improved by removing one of the highly correlated variables.\"\n",
    "* Feature Selection:<br>\n",
    "\"Feature selection is an important aspect of the model's accuracy. Wrong features, that does not give any/less information about the target variable, reduces the accuracy of the model drastically. Features can be properly selected by gaining some domain knowledge, visualization of the dataset, and PCA. PCA is a type of dimensionality reduction technique. There are various methods to reduce the dimensions (features) of training data like factor analysis, low variance, higher correlation, backward/ forward feature selection and others.\"\n",
    "* Cross Validation: <br>\n",
    "\"Cross validation technique is very important to calculate the expected mean squared error for the data. Cross Validation is one of the most important concepts in data modeling. It says, try to leave a sample on which you do not train the model and test the model on this sample before finalizing the model. This method helps us to achieve more generalized relationships.\"\n",
    "* Emsembling Methods:<br>\n",
    "\"This is the most common approach found majorly in winning solutions of Data science competitions. This technique simply combines the result of multiple weak models and produce better results. This can be achieved through many ways. It is always a better idea to apply ensemble methods to improve the accuracy of your model. There are two good reasons for this: a ) They are generally more complex than traditional methods. b) The traditional methods give you a good base level from which you can improve and draw from to create your ensembles.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__________________________________________________________________"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "* https://stackoverflow.com/questions/29600673/how-to-delete-columns-in-pyspark-dataframe -> drop a column\n",
    "* https://stackoverflow.com/questions/44627386/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe -> count the NA values\n",
    "* https://stackoverflow.com/questions/46956026/how-to-convert-column-with-string-type-to-int-form-in-pyspark-data-frame -> change the datatype of column\n",
    "* https://databricks.com/blog/2015/06/02/statistical-and-mathematical-functions-with-dataframes-in-spark.html -> calculating mean of a column\n",
    "* https://stackoverflow.com/questions/44773758/how-to-conditionally-replace-value-in-a-column-based-on-evaluation-of-expression -> how to replace a value in a column\n",
    "* https://stackoverflow.com/questions/38150885/looking-for-a-way-to-calculate-frequency-distribution-of-a-dataframe-in-spark-sc -> calculate frequency of words in a specific column\n",
    "* https://stackoverflow.com/questions/36942233/apply-stringindexer-to-several-columns-in-a-pyspark-dataframe -> convert non-numeric column to numeric using StringIndexer()\n",
    "* https://databricks-prod-cloudfront.cloud.databricks.com/public/4027ec902e239c93eaaa8714f173bcfc/3741049972324885/3783546674231736/4413065072037724/latest.html -> confusion matrix\n",
    "* https://mapr.com/blog/churn-prediction-pyspark-using-mllib-and-ml-packages/ -> confusion matrix\n",
    "* https://medium.com/@achilleus/spark-session-10d0d66d1d24 -> definitions for SparkSession\n",
    "* https://www.gangboard.com/blog/what-is-pyspark/ -> definition for PySpark\n",
    "* https://www.tutorialspoint.com/pyspark/pyspark_sparkcontext.htm -> definitions for SparkContext\n",
    "* https://www.geeksforgeeks.org/confusion-matrix-machine-learning/ -> confusion matrix\n",
    "* https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html -> bar graph\n",
    "* https://stackoverflow.com/questions/45177937/how-can-i-adapt-the-autolabel-function-in-matplotlib-so-that-it-displays-negativ -> bar graph\n",
    "* https://www.analyticsvidhya.com/blog/2015/12/improve-machine-learning-results/ -> improving the accuracy of the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
